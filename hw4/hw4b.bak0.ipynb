{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04dc0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae261d34",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4bdb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate_actor = 0.0003\n",
    "learning_rate_critic = 0.001\n",
    "gamma = 0.99  # Discount factor\n",
    "eps_clip = 0.2  # PPO clip parameter\n",
    "epochs = 10  # Number of epochs for PPO update\n",
    "batch_size = 64  # Size of a mini-batch for PPO update\n",
    "update_timestep = 2000  # Update policy every n timesteps\n",
    "max_ep_len = 500  # Max timesteps in one episode\n",
    "episodes = 2000  # Number of training episodes\n",
    "show_every = 100  # How often to print progress\n",
    "render = False\n",
    "\n",
    "# State discretization (number of buckets for each observation dimension)\n",
    "# (Cart Position, Cart Velocity, Pole Angle, Pole Velocity)\n",
    "num_buckets = (10, 10, 12, 12)\n",
    "\n",
    "# State bounds (limits)\n",
    "# (Cart Position, Cart Velocity, Pole Angle, Pole Velocity)\n",
    "state_bounds = [\n",
    "    (-4.8, 4.8),\n",
    "    (-4., 4.),\n",
    "    (-0.418, 0.418),  # 24 deg\n",
    "    (-4., 4.),\n",
    "]\n",
    "# ^these are for binning; these are not actual limits on the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5dd099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accf5b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(2)\n",
      "Observation Space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "State Dimensions: 4\n",
      "Action Dimensions: 2\n"
     ]
    }
   ],
   "source": [
    "# Environment initialization\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\" if render else None)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"State Dimensions: {state_dim}\")\n",
    "print(f\"Action Dimensions: {action_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f7ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class RolloutBuffer:\n",
    "    \"\"\"Stores transitions collected from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "        self.state_values = []  # Store critic values\n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "        del self.state_values[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33fc5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Defines the Actor-Critic network architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "        # Critic Network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state and returns action,\n",
    "        log probability, and state value.\n",
    "        \"\"\"\n",
    "        action_probs = self.actor(state)\n",
    "        dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_val = self.critic(state)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        \"\"\"\n",
    "        Evaluates the given state and action, returning log probability,\n",
    "        state value, and distribution entropy.\n",
    "        \"\"\"\n",
    "        action_probs = self.actor(state)\n",
    "        dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "\n",
    "        return action_logprobs, state_values, dist_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7df28338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \"\"\"Proximal Policy Optimization Agent.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr_actor,\n",
    "        lr_critic,\n",
    "        gamma,\n",
    "        epochs,\n",
    "        eps_clip,\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.epochs = epochs\n",
    "        self.eps_clip = eps_clip\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": self.policy.actor.parameters(), \"lr\": lr_actor},\n",
    "                {\"params\": self.policy.critic.parameters(), \"lr\": lr_critic},\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Selects an action using the old policy for data collection.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.actions.append(action)\n",
    "        self.buffer.logprobs.append(action_logprob)\n",
    "        self.buffer.state_values.append(state_val)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Updates the policy using PPO.\"\"\"\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal, value in zip(\n",
    "            reversed(self.buffer.rewards),\n",
    "            reversed(self.buffer.is_terminals),\n",
    "            reversed(self.buffer.state_values),\n",
    "        ):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            # Use critic value as baseline if not terminal, else use 0\n",
    "            # GAE (Generalized Advantage Estimation) could also be used here.\n",
    "            # Here we use a simpler approach: Returns = Rewards + gamma * V(s_next)\n",
    "            # If terminal, V(s_next) = 0. If not, V(s_next) is the stored critic value.\n",
    "            # Since we iterate backwards, V(s_next) is `discounted_reward`.\n",
    "            # We want Returns = Rewards + gamma * V(s_next)\n",
    "            # So, R_t = r_t + gamma * R_{t+1} if not terminal, or r_t if terminal.\n",
    "            # Alternatively, we calculate advantages: A_t = r_t + gamma * V(s_{t+1}) - V(s_t)\n",
    "            # And use Returns = A_t + V(s_t)\n",
    "            # Here we use Q_t ~ r_t + gamma * V(s_{t+1})\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # Convert list to tensor\n",
    "        old_states = (\n",
    "            torch.squeeze(torch.stack(self.buffer.states, dim=0))\n",
    "            .detach()\n",
    "            .to(device)\n",
    "        )\n",
    "        old_actions = (\n",
    "            torch.squeeze(torch.stack(self.buffer.actions, dim=0))\n",
    "            .detach()\n",
    "            .to(device)\n",
    "        )\n",
    "        old_logprobs = (\n",
    "            torch.squeeze(torch.stack(self.buffer.logprobs, dim=0))\n",
    "            .detach()\n",
    "            .to(device)\n",
    "        )\n",
    "        old_state_values = (\n",
    "            torch.squeeze(torch.stack(self.buffer.state_values, dim=0))\n",
    "            .detach()\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        # Calculate advantages: A_t = Rewards_t - V_old(s_t)\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.epochs):\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(\n",
    "                old_states, old_actions\n",
    "            )\n",
    "\n",
    "            # Match state_values tensor dimensions with rewards tensor for loss calculation\n",
    "            state_values = torch.squeeze(state_values)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = (\n",
    "                torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip)\n",
    "                * advantages\n",
    "            )\n",
    "\n",
    "            # Final loss of policy + Value loss + Entropy loss\n",
    "            # PPO-Clip loss + Value Function Loss - Entropy Bonus\n",
    "            loss = (\n",
    "                -torch.min(surr1, surr2)\n",
    "                + 0.5 * self.MseLoss(state_values, rewards)\n",
    "                - 0.01 * dist_entropy\n",
    "            )\n",
    "\n",
    "            # Take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # Clear buffer\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(\n",
    "            torch.load(\n",
    "                checkpoint_path, map_location=lambda storage, loc: storage\n",
    "            )\n",
    "        )\n",
    "        self.policy.load_state_dict(\n",
    "            torch.load(\n",
    "                checkpoint_path, map_location=lambda storage, loc: storage\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ad51f",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6347e164",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2000) must match the size of tensor b (2001) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     50\u001b[39m                 ppo_agent.buffer.rewards[-\u001b[32m1\u001b[39m] += (\n\u001b[32m     51\u001b[39m                     gamma * last_value.item()\n\u001b[32m     52\u001b[39m                 )  \u001b[38;5;66;03m# Add bootstrapped value\u001b[39;00m\n\u001b[32m     53\u001b[39m                 ppo_agent.buffer.is_terminals.append(\n\u001b[32m     54\u001b[39m                     \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     55\u001b[39m                 )  \u001b[38;5;66;03m# Add a dummy terminal flag\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m         \u001b[43mppo_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m         time_step = \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# Reset timestep counter after update\u001b[39;00m\n\u001b[32m     59\u001b[39m total_rewards.append(current_ep_reward)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mPPO.update\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     91\u001b[39m old_state_values = (\n\u001b[32m     92\u001b[39m     torch.squeeze(torch.stack(\u001b[38;5;28mself\u001b[39m.buffer.state_values, dim=\u001b[32m0\u001b[39m))\n\u001b[32m     93\u001b[39m     .detach()\n\u001b[32m     94\u001b[39m     .to(device)\n\u001b[32m     95\u001b[39m )\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Calculate advantages: A_t = Rewards_t - V_old(s_t)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m advantages = \u001b[43mrewards\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_state_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Optimize policy for K epochs\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.epochs):\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# Evaluating old actions and values\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (2000) must match the size of tensor b (2001) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "ppo_agent = PPO(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    learning_rate_actor,\n",
    "    learning_rate_critic,\n",
    "    gamma,\n",
    "    epochs,\n",
    "    eps_clip,\n",
    ")\n",
    "total_rewards = []\n",
    "highest_reward = 0\n",
    "time_step = 0\n",
    "episode_count = 0\n",
    "\n",
    "# Training loop\n",
    "for episode in range(1, episodes + 1):\n",
    "    state, info = env.reset()\n",
    "    current_ep_reward = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    render_this_episode = episode % show_every == 0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        # Select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(\n",
    "            terminated\n",
    "        )  # Store terminated, not truncated\n",
    "\n",
    "        time_step += 1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # Render\n",
    "        if render_this_episode and render:\n",
    "            env.render()\n",
    "\n",
    "        # Update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            # Need to add the value of the last state if not terminal\n",
    "            if not terminated and not truncated:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).to(device)\n",
    "                    _, _, last_value = ppo_agent.policy_old.act(state_tensor)\n",
    "                    ppo_agent.buffer.state_values.append(last_value)\n",
    "                    ppo_agent.buffer.rewards[-1] += (\n",
    "                        gamma * last_value.item()\n",
    "                    )  # Add bootstrapped value\n",
    "                    ppo_agent.buffer.is_terminals.append(\n",
    "                        False\n",
    "                    )  # Add a dummy terminal flag\n",
    "            ppo_agent.update()\n",
    "            time_step = 0  # Reset timestep counter after update\n",
    "\n",
    "    total_rewards.append(current_ep_reward)\n",
    "    if current_ep_reward > highest_reward:\n",
    "        highest_reward = current_ep_reward\n",
    "\n",
    "    # Logging\n",
    "    if episode % show_every == 0:\n",
    "        avg_reward = sum(total_rewards[-show_every:]) / len(\n",
    "            total_rewards[-show_every:]\n",
    "        )\n",
    "        print(\n",
    "            f\"Episode: {episode:5} | Avg Reward (last {show_every}): {avg_reward:6.2f} | Highest: {highest_reward:5.0f}\"\n",
    "        )\n",
    "        highest_reward = 0  # Reset highest for the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "data = {\n",
    "    \"policy_state_dict\": ppo_agent.policy.state_dict(),\n",
    "    \"total_rewards\": total_rewards,\n",
    "}\n",
    "with open(\"data_PPO.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d59e961",
   "metadata": {},
   "source": [
    "### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b230452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "with open(\"data_PPO.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    policy_state_dict = data[\"policy_state_dict\"]\n",
    "    total_rewards = data[\"total_rewards\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.plot(savgol_filter(total_rewards, 101, 3))  # Smoothed plot\n",
    "plt.plot(total_rewards, alpha=0.3)  # Raw plot\n",
    "plt.title(\"Total Rewards per Episode (Savgol Filtered)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.ylim([0, 600])  # CartPole-v1 max is 500, but allow some overshoot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render a single episode using the trained PPO agent\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "ppo_agent = PPO(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    learning_rate_actor,\n",
    "    learning_rate_critic,\n",
    "    gamma,\n",
    "    epochs,\n",
    "    eps_clip,\n",
    ")\n",
    "ppo_agent.policy_old.load_state_dict(policy_state_dict)\n",
    "\n",
    "observation, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not terminated and not truncated:\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(observation).to(device)\n",
    "        action_probs = ppo_agent.policy_old.actor(state_tensor)\n",
    "        # Select the most likely action (exploitation)\n",
    "        action = torch.argmax(action_probs).item()\n",
    "\n",
    "    next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "    observation = next_observation\n",
    "    time.sleep(0.02)  # 50 FPS max\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
